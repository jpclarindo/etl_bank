{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tr1eaFJpWH5P",
    "outputId": "8aa8ac16-4a1f-4bd6-e79b-8c4506299717"
   },
   "outputs": [],
   "source": [
    "# Necessário possuir a biblioteca DuckDB, holidays e babel instaladas.\n",
    "!pip install duckdb\n",
    "!pip install babel\n",
    "!pip install holidays\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyymwT9V7ehm",
    "outputId": "468843a9-d4e1-493f-807b-ba9616676db4"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Getting Github files if this notebook is executed in a Google Colab environment.\n",
    "if 'google.colab' in sys.modules:\n",
    "  temp_folder = \"etl_bank\"\n",
    "\n",
    "  !git clone -b 'dev' 'https://github.com/jpclarindo/etl_bank.git' $temp_folder\n",
    "  !rsync -a $temp_folder/ .\n",
    "  !rm -rf $temp_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovHkkl2sndSc"
   },
   "source": [
    "Important! Google Drive Link for data source is required to execute this notebook. Please, put the link in gdrive_link.txt at the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y15PykOQV7QI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import src.utils as utils\n",
    "\n",
    "# Libraries to create date dimension\n",
    "from datetime import datetime, timedelta\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "import holidays\n",
    "\n",
    "# Auxiliar libraries\n",
    "import time\n",
    "import json\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0DARAeUyXUl1"
   },
   "outputs": [],
   "source": [
    "# Init DuckDB connection.\n",
    "con = duckdb.connect(database='database.duckdb', read_only=False)\n",
    "\n",
    "# Creating cleaning function mapping for each attribute\n",
    "cleaning_mapping = json.load(open('var/function_mapping.json','r',encoding='utf-8'))\n",
    "\n",
    "# Creating date mapping\n",
    "date_mapping = json.load(open('var/date_mapping.json','r',encoding='utf-8'))\n",
    "\n",
    "# Creating relationships mapping\n",
    "relationships = json.load(open('var/relationship_mapping.json','r',encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK375AmzY9_E"
   },
   "source": [
    "Creating time and date dimension tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "UPpqRpg-4X5u"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "def create_time_dimension_table():\n",
    "  time_list = []\n",
    "  time_of_day = ['Early Morning', 'Morning', 'Afternoon', 'Night']\n",
    "  time_of_day_pt = ['Madrugada', 'Manhã', 'Tarde', 'Noite']\n",
    "\n",
    "  for i in range(86400):\n",
    "    min_sec = i % 3600\n",
    "\n",
    "    time_dict = {'tempo_id': i+1,\n",
    "                'hora': i // 3600,\n",
    "                'minuto': min_sec // 60,\n",
    "                'segundo': min_sec % 60\n",
    "                }\n",
    "\n",
    "    time_dict['periodo_en'] = time_of_day[time_dict['hora'] // 6]\n",
    "    time_dict['periodo_pt'] = time_of_day_pt[time_dict['hora'] // 6]\n",
    "    time_dict['formatado'] = f\"{time_dict['hora']:02d}:{time_dict['minuto']:02d}:{time_dict['segundo']:02d}\"\n",
    "\n",
    "    time_list.append(time_dict)\n",
    "\n",
    "\n",
    "  time_df = pd.DataFrame(time_list)\n",
    "  con.execute(\"\"\"CREATE TABLE IF NOT EXISTS d_tempo\n",
    "                 AS SELECT * FROM time_df\"\"\")\n",
    "\n",
    "  print('Time dimension created and inserted in the database')\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "uSoC-AY0ZCgl"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "def create_date_dimension_table():\n",
    "  date_list = []\n",
    "  start_date =  datetime(1950, 1, 1)\n",
    "  end_date = datetime(2050, 12, 31)\n",
    "  delta = timedelta(days=1)\n",
    "  count = 0\n",
    "\n",
    "  while start_date <= end_date:\n",
    "    count += 1\n",
    "\n",
    "    date_dict = {'data_id': count,\n",
    "                'formatado': start_date.strftime('%Y-%m-%d'),\n",
    "                'ano': start_date.year,\n",
    "                'trimestre': format_date(start_date,'Q'),\n",
    "                'nome_trimestre_en': format_date(start_date, format='QQQQ', locale='en_US'),\n",
    "                'nome_trimestre_pt': format_date(start_date, format='QQQQ', locale='pt_BR'),\n",
    "                'mes': start_date.month,\n",
    "                'nome_mes_en': format_date(start_date, format='MMMM', locale='en_US'),\n",
    "                'nome_mes_pt': format_date(start_date, format='MMMM', locale='pt_BR'),\n",
    "                'dia': start_date.day,\n",
    "                'nome_dia_en': format_date(start_date, format='EEEE', locale='en_US'),\n",
    "                'nome_dia_pt': format_date(start_date, format='EEEE', locale='pt_BR'),\n",
    "                'fim_de_semana': 1 if start_date.weekday() in [5, 6] else 0,\n",
    "                'feriado': 1 if start_date.strftime('%Y-%m-%d') in holidays.Brazil() else 0\n",
    "    }\n",
    "\n",
    "    print(date_dict)\n",
    "\n",
    "    start_date += delta\n",
    "    date_list.append(date_dict)\n",
    "\n",
    "  date_df = pd.DataFrame(date_list)\n",
    "  con.execute(\"\"\"CREATE TABLE IF NOT EXISTS d_data\n",
    "                 AS SELECT * FROM date_df\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xRpW9gXe1Al"
   },
   "source": [
    "ETL Processing - Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-u3PGCFqez3N",
    "outputId": "8a2828ad-fe17-45e1-f4e3-f2b2663f99b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16jF2hJHVOwikmgoEKs9wARnrE_j6Y7e2\n",
      "To: C:\\Users\\jpaul\\OneDrive\\Documentos\\GitHub\\etl_bank\\data.zip\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.24M/1.24M [00:00<00:00, 3.79MB/s]\n"
     ]
    }
   ],
   "source": [
    "def get_raw_data():\n",
    "  #\n",
    "  utils.download_data()\n",
    "\n",
    "  # Adding files through a dict\n",
    "  dfs_raw = {}\n",
    "\n",
    "  file_list = glob.glob('data/*.csv')\n",
    "\n",
    "  for csv_file in file_list:\n",
    "    table_name = os.path.basename(csv_file).replace('.csv','')\n",
    "    date_columns = date_mapping.get(table_name,None)\n",
    "\n",
    "    df = pd.read_csv(csv_file, sep=',', parse_dates=date_columns, encoding='utf-8')\n",
    "\n",
    "    # Metadata for extraction\n",
    "    df['_load_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['_file_name'] = os.path.basename(csv_file)\n",
    "\n",
    "    dfs_raw[table_name] = df\n",
    "\n",
    "  return dfs_raw\n",
    "\n",
    "dfs_raw = get_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x04Nf87aCUll"
   },
   "source": [
    "ETL - Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Pd8p9tJfA4W-"
   },
   "outputs": [],
   "source": [
    "def cleaning_attributes(df):\n",
    "    df_clean = df.copy()\n",
    "    current_columns = set(df_clean.columns)\n",
    "\n",
    "    for column, function_name in cleaning_mapping.items():\n",
    "        if column in current_columns:\n",
    "            if hasattr(utils, function_name):\n",
    "                func = getattr(utils, function_name)\n",
    "                print(f\"   -> Expanding column '{column}' using '{function_name}'\")\n",
    "\n",
    "                # Aplica a função que retorna dict\n",
    "                function_results = df_clean[column].apply(func)\n",
    "\n",
    "                sample = function_results.dropna().iloc[0] if not function_results.dropna().empty else None\n",
    "\n",
    "                if isinstance(sample, dict):\n",
    "                # Transforma a coluna de dicts em um novo DataFrame de colunas\n",
    "                    df_expanded = pd.json_normalize(function_results)\n",
    "                    df_clean = pd.concat([df_clean, df_expanded], axis=1)\n",
    "                    df_clean = df_clean.loc[:, ~df_clean.columns.duplicated(keep='last')]\n",
    "                else:\n",
    "                    df_clean[column] = function_results\n",
    "                    \n",
    "                #df_clean.drop(columns=[column], inplace=True)\n",
    "            else:\n",
    "                print(f\"Error\")       \n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "E-Lkvu-CFavk",
    "outputId": "c907844b-a7fc-4fe7-ea8b-5a1d4f26ca59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning agencias attributes\n",
      "   -> Expanding column 'tipo_agencia' using 'clean_text'\n",
      "   -> Expanding column 'cidade' using 'clean_text'\n",
      "   -> Expanding column 'uf' using 'clean_text'\n",
      "   -> Expanding column 'endereco' using 'get_address_dict'\n",
      "Cleaning clientes attributes\n",
      "   -> Expanding column 'primeiro_nome' using 'clean_text'\n",
      "   -> Expanding column 'ultimo_nome' using 'clean_text'\n",
      "   -> Expanding column 'tipo_cliente' using 'clean_text'\n",
      "   -> Expanding column 'email' using 'normalize_email'\n",
      "   -> Expanding column 'cep' using 'normalize_cep'\n",
      "   -> Expanding column 'endereco' using 'get_address_dict'\n",
      "   -> Expanding column 'data_nascimento' using 'get_age_dict'\n",
      "Cleaning colaboradores attributes\n",
      "   -> Expanding column 'primeiro_nome' using 'clean_text'\n",
      "   -> Expanding column 'ultimo_nome' using 'clean_text'\n",
      "   -> Expanding column 'email' using 'normalize_email'\n",
      "   -> Expanding column 'cep' using 'normalize_cep'\n",
      "   -> Expanding column 'endereco' using 'get_address_dict'\n",
      "   -> Expanding column 'data_nascimento' using 'get_age_dict'\n",
      "Cleaning colaborador_agencia attributes\n",
      "Cleaning contas attributes\n",
      "   -> Expanding column 'tipo_conta' using 'clean_text'\n",
      "Cleaning propostas_credito attributes\n",
      "   -> Expanding column 'status_proposta' using 'clean_text'\n",
      "Cleaning transacoes attributes\n",
      "   -> Expanding column 'nome_transacao' using 'clean_text'\n"
     ]
    }
   ],
   "source": [
    "dfs_clean = {}\n",
    "\n",
    "for table_name, df_raw in dfs_raw.items():\n",
    "    print(f'Cleaning {table_name} attributes')\n",
    "    dfs_clean[table_name] = cleaning_attributes(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL - Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_referential_integrity(dfs_dict_original, relationships_config):\n",
    "    dfs_dict = dfs_dict_original.copy()\n",
    "    counter = 0\n",
    "    \n",
    "    for rule in relationships_config:       \n",
    "        source_col = rule['source_column']\n",
    "        target_col = rule['target_column']\n",
    "        \n",
    "        df_source = dfs_dict.get(rule['source_table'])\n",
    "        df_target = dfs_dict.get(rule['target_table'])\n",
    "        \n",
    "        # Identifying orphan keys\n",
    "        source_keys = set(df_source[source_col].dropna().unique())\n",
    "        target_keys = set(df_target[target_col].dropna().unique())\n",
    "        orphan_keys = list(source_keys - target_keys)\n",
    "        \n",
    "        if not orphan_keys:\n",
    "            print(f'The relationship between {rule['target_table']} and {rule['source_table']} complains integrity')\n",
    "            counter += 1\n",
    "\n",
    "            # If all relationships are consistent, return a boolean (for checking)\n",
    "            if counter < len(relationships_config):\n",
    "                continue\n",
    "            else:\n",
    "                return True\n",
    "                \n",
    "        print(f\"Found inconsistencies ({len(orphan_keys)} rows) in '{rule['target_column']}' (Source table: {rule['source_table']})\")\n",
    "\n",
    "        # Creating a new dataframe with orphan keys\n",
    "        df_orphans = pd.DataFrame({target_col: orphan_keys})\n",
    "        \n",
    "        for col in df_target.columns:\n",
    "            if col == target_col:\n",
    "                continue             \n",
    "\n",
    "            # Get the type\n",
    "            current_dtype = df_target[col].dtype\n",
    "            \n",
    "            # Fill the row with NI values\n",
    "            if pd.api.types.is_numeric_dtype(current_dtype):\n",
    "                df_orphans[col] = -1\n",
    "            elif pd.api.types.is_datetime64_any_dtype(current_dtype):\n",
    "                df_orphans[col] = pd.Timestamp('1900-01-01')\n",
    "            else:\n",
    "                df_orphans[col] = 'NI'\n",
    "\n",
    "        dfs_dict[rule['target_table']] = pd.concat([df_target, df_orphans], ignore_index=True)\n",
    "\n",
    "    return dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relationship between contas and transacoes complains integrity\n",
      "Found inconsistencies (1 rows) in 'cod_cliente' (Source table: contas)\n",
      "The relationship between agencias and contas complains integrity\n",
      "The relationship between clientes and propostas_credito complains integrity\n",
      "The relationship between colaboradores and propostas_credito complains integrity\n",
      "The relationship between agencias and colaborador_agencia complains integrity\n"
     ]
    }
   ],
   "source": [
    "dfs_dict_checked = repair_referential_integrity(dfs_clean, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relationship between contas and transacoes complains integrity\n",
      "The relationship between clientes and contas complains integrity\n",
      "The relationship between agencias and contas complains integrity\n",
      "The relationship between clientes and propostas_credito complains integrity\n",
      "The relationship between colaboradores and propostas_credito complains integrity\n",
      "The relationship between agencias and colaborador_agencia complains integrity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repair_referential_integrity(dfs_dict_checked, relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL - Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
